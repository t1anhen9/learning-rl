import os
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np
from tqdm import tqdm

# ===== è®¾å¤‡é€‰æ‹© =====
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ===== ç­–ç•¥ç½‘ç»œ =====
class PolicyNet(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, x):
        return self.net(x)  # ç›´æ¥è¾“å‡º logits

# ===== é‡‡æ ·è½¨è¿¹ =====
def sample_trajectory(env, policy, max_steps=1000, min_reward=None):
    obs, _ = env.reset()
    obs_list, act_list, logp_list = [], [], []
    total_reward = 0

    for _ in range(max_steps):
        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)
        logits = policy(obs_tensor)
        dist = Categorical(logits=logits)
        action = dist.sample()
        logp = dist.log_prob(action)

        next_obs, reward, terminated, truncated, _ = env.step(action.item())

        obs_list.append(obs_tensor)
        act_list.append(action)
        logp_list.append(logp)
        total_reward += reward
        obs = next_obs

        if terminated or truncated:
            break

    # è·³è¿‡è¿‡çŸ­æˆ–æå·®çš„è½¨è¿¹
    if min_reward is not None and total_reward < min_reward:
        return None

    return {
        "observations": torch.stack(obs_list),
        "actions": torch.stack(act_list),
        "log_probs": torch.stack(logp_list),
        "reward": total_reward
    }

# ===== DPO æŸå¤±å‡½æ•° =====
def dpo_loss(traj_a, traj_b, beta=1.0):
    logp_a = traj_a["log_probs"].sum()
    logp_b = traj_b["log_probs"].sum()
    logits = beta * torch.stack([logp_a, logp_b])
    pref_logp = torch.log_softmax(logits, dim=0)[0]  # a æ›´ä¼˜
    return -pref_logp

# ===== ç¯å¢ƒä¸ç­–ç•¥åˆå§‹åŒ– =====
env = gym.make("LunarLander-v2")
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
policy = PolicyNet(state_dim, action_dim).to(device)
optimizer = optim.Adam(policy.parameters(), lr=1e-4)

# ===== è®­ç»ƒå‚æ•° =====
episodes = 3000
batch_size = 8
beta = 1.0
min_reward = -200  # æœ€ä½ reward ç­›é€‰ï¼Œé˜²æ­¢éå¸¸å¤±è´¥çš„è½¨è¿¹è¿›å…¥ DPO
use_pretrain = True
pretrain_steps = 5000

# ===== å¯é€‰ REINFORCE é¢„è®­ç»ƒ =====
if use_pretrain:
    print("ğŸ”§ Running REINFORCE warm-up...")
    for step in tqdm(range(pretrain_steps)):
        traj = sample_trajectory(env, policy)
        if traj is None:
            continue
        loss = -traj["log_probs"].sum() * traj["reward"] / 100.0  # ç®€å•åŠ æƒ
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# ===== ä¸»è®­ç»ƒå¾ªç¯ï¼ˆDPOï¼‰=====
print("ğŸš€ Starting DPO training...")
for epoch in tqdm(range(episodes)):
    pairs = []
    while len(pairs) < batch_size:
        traj1 = sample_trajectory(env, policy, min_reward=min_reward)
        traj2 = sample_trajectory(env, policy, min_reward=min_reward)
        if traj1 is None or traj2 is None:
            continue

        if traj1["reward"] >= traj2["reward"]:
            traj_a, traj_b = traj1, traj2
        else:
            traj_a, traj_b = traj2, traj1
        pairs.append((traj_a, traj_b))

    losses = []
    for traj_a, traj_b in pairs:
        loss = dpo_loss(traj_a, traj_b, beta=beta)
        losses.append(loss)

    total_loss = torch.stack(losses).mean()

    if torch.isnan(total_loss):
        print("âš ï¸ Loss became NaN. Stopping training.")
        break

    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f"Epoch {epoch}, DPO loss: {total_loss.item():.3f}")

env.close()

# ===== è§†é¢‘ä¿å­˜ï¼ˆä»…æµ‹è¯•ä¸€æ¬¡ï¼‰=====
from gymnasium.wrappers import RecordVideo

# åˆ›å»ºç¯å¢ƒå¹¶åŒ…è£…è§†é¢‘è®°å½•å™¨
env = gym.make("LunarLander-v2", render_mode="rgb_array")
video_dir = "./videos"  # æ›¿æ¢ä¸ºä½ å¸Œæœ›ä¿å­˜è§†é¢‘çš„è·¯å¾„
env = RecordVideo(
    env, 
    video_dir, 
    episode_trigger=lambda episode_id: True,  # æ¯ä¸€å±€éƒ½å½•
    name_prefix="dpo_lander_final"
)

num_episodes = 5  # ä½ æƒ³ç©çš„æ¬¡æ•°

for episode in range(num_episodes):
    obs, _ = env.reset()
    done = False

    while not done:
        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)
        logits = policy(obs_tensor)
        dist = Categorical(logits=logits)
        action = dist.sample().item()

        obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

env.close()
print(f"ğŸ¬ æ‰€æœ‰è§†é¢‘å·²ä¿å­˜åˆ°ï¼š{video_dir}")